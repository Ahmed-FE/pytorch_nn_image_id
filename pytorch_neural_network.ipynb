{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is to get the image from the data set in torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set=torchvision.datasets.FashionMNIST(\n",
    "     root='./data/FashionMNIST'\n",
    "     ,train=True\n",
    "    ,download=True\n",
    "     ,transform=transforms.Compose([transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here is where we are defining our layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # convolutional layers to caputre the charachterstic of the picture followed by a fully connected layer \n",
    "        self.conv1=nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3))\n",
    "        self.conv2=nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3))\n",
    "        self.fc1=nn.Linear(in_features=1600, out_features=128)\n",
    "        self.out=nn.Linear(in_features=128, out_features=10)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # (1) input layer\n",
    "        x=x\n",
    "        print(x.shape)\n",
    "        # (2) hidden conv layer\n",
    "        x=self.conv1(x)\n",
    "        print(x.shape)\n",
    "        ## the Relu function and max pool 2d (the activation functions)\n",
    "        ## them directly from the forward function \n",
    "        x=F.relu(x)\n",
    "        x=F.max_pool2d(x, kernel_size=2,stride=2)\n",
    "        print(x.shape)\n",
    "        # (3)hidden conv layer\n",
    "        x=self.conv2(x)\n",
    "        x=F.relu(x)\n",
    "        x=F.max_pool2d(x, kernel_size=2,stride=2)\n",
    "        print(x.shape)\n",
    "        \n",
    "        # (4) the hidden Linear layer \n",
    "        x=x.reshape(-1,1600)\n",
    "        print(x.shape)\n",
    "        x=self.fc1(x)\n",
    "        x=F.dropout(x,0.5)\n",
    "        \n",
    "        # (5) the output layer \n",
    "        x=self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1b329e5cca0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we will run our class. the way we defined the nural network using nn.module made it inherit all the charactristic of the nural network from pytorch**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data/FashionMNIST\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "conv_Neural_netwrok=CNN()\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=next(iter(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label=sample\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need a patch with size 1 to be able to pass the image to the network and get prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 32, 26, 26])\n",
      "torch.Size([1, 32, 13, 13])\n",
      "torch.Size([1, 64, 5, 5])\n",
      "torch.Size([1, 1600])\n"
     ]
    }
   ],
   "source": [
    "pred=conv_Neural_netwrok(image.unsqueeze(0))      # image size needs to be (patch *in_channels*H*W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patch some images to pass it to the network and start working with this images. \n",
    "so now we use dataloader from pytorch to patch our data. the whole training sample are 6000 samples but for this project and for sake of simplicity we will work with 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "     train_set ,batch_size=10  \n",
    ")  # the default for batch_size=1  so if you want like 100 samples you need to define it . also our data is 60000 \n",
    "# so try batching 1000000 will not get you more than the 60000 in our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10, 32, 26, 26])\n",
      "torch.Size([10, 32, 13, 13])\n",
      "torch.Size([10, 64, 5, 5])\n",
      "torch.Size([10, 1600])\n"
     ]
    }
   ],
   "source": [
    "pred=conv_Neural_netwrok(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This prints in the code is just for the sake of debugging but that can be done using spyder debugging methods with ipdb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* first we started with a patch of 10 images each one of tem is grey image that is why the color is just one and it has a size of 28 height with 28 width \n",
    "* the first convolutional is filtering it using 32 filter each with size 3 . the result of this convolution is 32  image with size (26*26)\n",
    "* and as a result of the max pool operation and the convolution the size drop more to be 64 filter each with 5 *5 .\n",
    "* so when it come to make the fullt connected layer we can easier flatten the photo using the flatten input method or we can just change the size again to be 1 filter but combinning this 64 filter with 5 * 5 size into one big tensor again that requires tensorwith size 64*5*5 again that is why we need 1600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compare labels against the max achieved prediction we can use eq function \n",
    "res.eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
